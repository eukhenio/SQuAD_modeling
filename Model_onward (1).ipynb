{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model_onward.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1eYJm-c302T571BQhUbOdFk1iwEzMM5KD","authorship_tag":"ABX9TyMbz4ytaJdwC0qVsmbRej9o"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"nujaDsLmzlFM","colab_type":"code","outputId":"ab4b2f5b-4aa2-4568-b851-1086bdc0b781","executionInfo":{"status":"ok","timestamp":1592197654624,"user_tz":-180,"elapsed":3250,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import re\n","import numpy as np\n","import pickle\n","import json\n","\n","#from keras.preprocessing.sequence import pad_sequences\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Bidirectional, Dropout, Embedding, LSTM\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from tensorflow.keras.metrics import CosineSimilarity\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","#from keras.layers import recurrent\n","\n","#from keras.layers.embeddings import Embedding\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint\n","\n","#from sklearn.metrics import hamming_loss, precision_score,\\\n","#                            recall_score, accuracy_score\n","#from sklearn.metrics.pairwise import cosine_distances\n","#from scipy.spatial.distance import cosine"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BReQm8Khlgyl","colab_type":"code","colab":{}},"source":["def tokenize(sent):\n","  return [x.strip() for x in re.split(r'(\\W+)', sent) if x.strip()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCATb8ikll2B","colab_type":"code","outputId":"f7b2884a-5c41-476f-bbd3-485e891c13cc","executionInfo":{"status":"ok","timestamp":1592197669809,"user_tz":-180,"elapsed":5162,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with open('drive/My Drive/final_project/train-v2.0.json', 'r') as f:\n","  content = json.loads(f.read())\n","type(content)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"n7E_duTvlrC9","colab_type":"code","outputId":"d4b7fdb8-1c88-42a7-d3cc-9170503b5fda","executionInfo":{"status":"ok","timestamp":1592197674465,"user_tz":-180,"elapsed":1818,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data = content['data']\n","data[0].keys()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['title', 'paragraphs'])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"2S6PSGI4lu2A","colab_type":"code","colab":{}},"source":["def parse_data(data):\n","  vocab_set = set()\n","  vocab = {}\n","  triplex_list = []\n","  context_list = []\n","  question_list = []\n","  answer_list = []\n","  original_answer_list = []\n","\n","\n","  # Context and questions extracting\n","  for topic in data:\n","    for part in topic['paragraphs']:\n","      blocks = part['qas']\n","      for block in blocks:\n","        if len(block['answers']) == 1:\n","          context = part['context']\n","          vocab_set |= set(tokenize(context))\n","          context_list.append(context)\n","          vocab_set |= set(tokenize(block['question']))\n","          question_list.append(block['question'])\n","  \n","  # Making dictionary with shape {'token': number}, where numbers are in range 1..\n","  i = 1\n","  for token in vocab_set:\n","    vocab[token] = i\n","    i += 1\n","  \n","  # Context vectorization and finding of context_maxlen\n","  context_vectors = []\n","  context_maxlen = 0\n","  for context in context_list:\n","    vectorized_context = []\n","    tokens = tokenize(context)\n","    for token in tokens:\n","      vectorized_context.append(vocab[token])\n","    context_vectors.append(vectorized_context)\n","    if len(tokens) > context_maxlen:\n","      context_maxlen = len(tokens)\n","  context_vectors = pad_sequences(context_vectors, maxlen=context_maxlen, padding='post')\n","  \n","  # Answer extracting\n","  for topic in data:\n","    for part in topic['paragraphs']:\n","      blocks = part['qas']\n","      for block in blocks:\n","        if len(block['answers']) == 1:\n","          context = part['context']\n","          tokens = tokenize(context)\n","          answer_vector = np.zeros(len(tokens))\n","          answer_start = block['answers'][0]['answer_start']\n","          text = block['answers'][0]['text']\n","          before_answer = context[:answer_start]\n","          tokens_before = tokenize(before_answer)\n","          answer_symbols = context[answer_start:answer_start + len(text)]\n","          answer_tokens = tokenize(answer_symbols)\n","          if answer_tokens != tokenize(text):\n","            print('Mistake')\n","            break\n","          answer_vector[len(tokens_before): len(tokens_before) + len(answer_tokens)] = 1\n","          original_answer_list.append(text)\n","          answer_list.append(answer_vector)\n","  \n","  answer_vectors = pad_sequences(answer_list, maxlen=context_maxlen, padding='post')\n","  \n","  # Question vectorization and question_maxlen finding \n","  question_vectors = []\n","  question_maxlen = 0\n","  for question in question_list:\n","    vectorized_question = []\n","    tokens = tokenize(question)\n","    for token in tokens:\n","      vectorized_question.append(vocab[token])\n","    question_vectors.append(vectorized_question)\n","    if len(tokens) > question_maxlen:\n","      question_maxlen = len(tokens)\n","  question_vectors = pad_sequences(question_vectors, maxlen=question_maxlen, padding='post')\n","\n","  return context_vectors, question_vectors, answer_vectors,\\\n","   vocab, context_maxlen, question_maxlen, context_list, question_list, answer_list, original_answer_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7g1W5GRitqg","colab_type":"code","colab":{}},"source":["context_vectors, question_vectors, answer_vectors, vocab, context_maxlen,\\\n"," question_maxlen, context_list, question_list, answer_list, original_answer_list = parse_data(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJv7DgzslmZL","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","context_train, context_test, question_train, question_test,\\\n","  answer_train, answer_test = train_test_split(context_vectors,\n","  question_vectors, answer_vectors, shuffle=False, test_size=0.2, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOlXQboemOTm","colab_type":"code","colab":{}},"source":["RNN = recurrent.LSTM\n","EMBED_HIDDEN_SIZE = 100\n","CONTEXT_HIDDEN_SIZE = 200\n","QUESTION_HIDDEN_SIZE = 200\n","\n","vocab_size = len(vocab) + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2gikt-dmbhG","colab_type":"code","outputId":"8d9cf8ab-9263-4a94-d48f-0523279d746e","executionInfo":{"status":"ok","timestamp":1592197738210,"user_tz":-180,"elapsed":14440,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["glove_dir = 'drive/My Drive/final_project'\n","embeddings_index = {}\n","f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:], dtype='float32')\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","len(embeddings_index)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["400000"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"iPGBrDUsmgZ9","colab_type":"code","outputId":"06f74e07-31cf-4202-d39f-e8a60b955019","executionInfo":{"status":"ok","timestamp":1592197741968,"user_tz":-180,"elapsed":759,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["embedding_matrix = np.zeros((vocab_size, EMBED_HIDDEN_SIZE))\n","for word, i in vocab.items():\n","  embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector\n","\n","embedding_matrix.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(96570, 100)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"E_YGqcGvaZah","colab_type":"code","outputId":"977422a0-d2ce-42f2-9c83-6f6545ce0a17","executionInfo":{"status":"ok","timestamp":1592197749924,"user_tz":-180,"elapsed":1962,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# build model\n","\n","context = layers.Input(shape=(context_maxlen,), dtype='int32')\n","encoded_context = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(context)\n","_context = layers.Bidirectional(RNN(CONTEXT_HIDDEN_SIZE))(encoded_context)\n","dropout_1 = layers.Dropout(0.33)(_context)\n","\n","question = layers.Input(shape=(question_maxlen,), dtype='int32')\n","encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n","_question = layers.Bidirectional(RNN(QUESTION_HIDDEN_SIZE))(encoded_question)\n","dropout_2 = layers.Dropout(0.33)(_question)\n","\n","merged = layers.concatenate([dropout_1, dropout_2])\n","dropout_ = layers.Dropout(0.33)(merged)\n","preds = layers.Dense(context_maxlen, activation='sigmoid')(dropout_)\n","\n","\n","model = Model([context, question], preds)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Build model...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-d0ebNV8m3t7","colab_type":"code","colab":{}},"source":["model.layers[2].set_weights([embedding_matrix])\n","model.layers[2].trainable = False\n","\n","model.layers[3].set_weights([embedding_matrix])\n","model.layers[3].trainable = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYtidhPGIGb-","colab_type":"code","outputId":"82c19bf6-a105-4334-e25c-8ef9c8874e24","executionInfo":{"status":"ok","timestamp":1592197791298,"user_tz":-180,"elapsed":747,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":544}},"source":["optimizer = keras.optimizers.RMSprop(learning_rate=0.005)\n","\n","model.compile(optimizer=optimizer,\n","              loss=BinaryCrossentropy(),\n","              metrics=['accuracy', \\\n","                       CosineSimilarity(axis=1)])\n","\n","model.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 844)          0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 60)           0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 844, 100)     9657000     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 60, 100)      9657000     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 400)          481600      embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","bidirectional_2 (Bidirectional) (None, 400)          481600      embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 400)          0           bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 400)          0           bidirectional_2[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 800)          0           dropout_1[0][0]                  \n","                                                                 dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 800)          0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 844)          676044      dropout_3[0][0]                  \n","==================================================================================================\n","Total params: 20,953,244\n","Trainable params: 1,639,244\n","Non-trainable params: 19,314,000\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5KBZurVxtdez","colab_type":"code","colab":{}},"source":["with open(json_file, 'r') as f:\n","  loaded_model = model_from_json(f.read())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8FBlfD7xgmD","colab_type":"code","colab":{}},"source":["model.load_weights('/content/drive/My Drive/final_project/weights_file_10.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3iXJVejHnqY0","colab_type":"code","colab":{}},"source":["# not for run\n","\n","#opt = tf.keras.optimizers.RMSprop(learning_rate=0.005)\n","\n","#lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n","#    initial_learning_rate=1e-2,\n","#    decay_steps=10000,\n","#    decay_rate=0.9)\n","#optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n","\n","#model.compile(optimizer=opt,\n","#              loss=BinaryCrossentropy(),\n","#              metrics=['accuracy', \\\n","#                       CosineSimilarity(axis=1)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIqMC0-VnCmv","colab_type":"code","outputId":"ffc0fe6f-2c28-4a52-f60a-95489b223af5","executionInfo":{"status":"ok","timestamp":1592211917118,"user_tz":-180,"elapsed":14109190,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["BATCH_SIZE = 512\n","EPOCHS = 3\n","\n","callback = ModelCheckpoint(filepath='/content/drive/My Drive/final_project/weights_file_11.h5',\n","              monitor='val_loss',\n","              mode='auto',\n","              save_best_only=True)\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n","                              patience=3, min_lr=0.001)\n","\n","\n","print('Training')\n","history = model.fit([context_train, question_train], answer_train,\n","          batch_size=BATCH_SIZE,\n","          epochs=EPOCHS,\n","          validation_split=0.05,\n","          callbacks=[callback, reduce_lr])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Training\n","Train on 65983 samples, validate on 3473 samples\n","Epoch 1/3\n","65983/65983 [==============================] - 4681s 71ms/step - loss: 0.0167 - accuracy: 0.0373 - cosine_similarity: 0.2096 - val_loss: 0.0312 - val_accuracy: 0.0153 - val_cosine_similarity: 0.2070\n","Epoch 2/3\n","65983/65983 [==============================] - 4743s 72ms/step - loss: 0.0163 - accuracy: 0.0389 - cosine_similarity: 0.2094 - val_loss: 0.0310 - val_accuracy: 0.0144 - val_cosine_similarity: 0.2097\n","Epoch 3/3\n","65983/65983 [==============================] - 4678s 71ms/step - loss: 0.0162 - accuracy: 0.0394 - cosine_similarity: 0.2112 - val_loss: 0.0311 - val_accuracy: 0.0153 - val_cosine_similarity: 0.2118\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NTv1wAlF_kmW","colab_type":"code","colab":{}},"source":["history_dict = history.history\n","lr = history_dict['lr']\n","lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNgWh2QtmOkX","colab_type":"code","colab":{}},"source":["loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","acc = history_dict['accuracy']\n","val_acc = history_dict['val_accuracy']\n","cosine_similarity=history_dict['cosine_similarity']\n","val_cosine_similarity=history_dict['val_cosine_similarity']\n","\n","epochs = list(range(1, len(acc) + 1))\n","\n","import plotly.graph_objs as go\n","from plotly.offline import iplot\n","\n","trace0 = go.Scatter(\n","    x = epochs,\n","    y = loss_values,\n","    mode = 'lines',\n","    name = 'training loss'\n",")\n","trace1 = go.Scatter(\n","    x = epochs,\n","    y = val_loss_values,\n","    mode = 'lines',\n","    name = 'validation loss'\n",")\n","trace2 = go.Scatter(\n","    x = epochs,\n","    y = cosine_similarity,\n","    mode = 'lines',\n","    name = 'cosine_similarity'\n",")\n","trace3 = go.Scatter(\n","    x = epochs,\n","    y = val_cosine_similarity,\n","    mode = 'lines',\n","    name = 'val_cosine_similarity'\n",")\n","\n","data_1 = [trace0, trace1]\n","\n","layout_1 = {'title': 'Config.1 Training and validation loss', 'xaxis': {'title': 'epochs'}, 'yaxis': {'title': 'loss'}}\n","fig_1 = go.Figure(data=data_1, layout=layout_1)\n","iplot(fig_1, show_link=False)\n","\n","data_2 = [trace2, trace3]\n","\n","layout_2 = {'title': 'Config.1 Training and validation cosine_similarity', 'xaxis': {'title': 'epochs'}, 'yaxis': {'title': 'cosine_similarity'}}\n","fig_2 = go.Figure(data=data_2, layout=layout_2)\n","iplot(fig_2, show_link=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jU36ngDxYwMX","colab_type":"code","colab":{}},"source":["reversed_vocab = {}\n","for word in vocab:\n","  reversed_vocab[vocab[word]] = word\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVEsFtD_UmVv","colab_type":"code","colab":{}},"source":["def get_words(number):\n","\n","  original_context = context_list[number]\n","  context_vec = context_train[number]\n","  restored_context = []\n","  for i in context_vec:\n","    if i != 0:\n","      restored_context.append(reversed_vocab[i])\n","  print('original_context: ', original_context)\n","  print('restored_context: ', restored_context)\n","\n","  original_question = question_list[number]\n","  question_vec = question_train[number]\n","  restored_question = []\n","  for i in question_vec:\n","    if i != 0:\n","      restored_question.append(reversed_vocab[i])\n","  print('original_question: ', original_question)\n","  print('restored_question: ', restored_question)\n","                            \n","\n","  original_answer = original_answer_list[number]\n","  context = context_vec.reshape(1, 844)\n","  question = question_vec.reshape(1, 60)\n","  answer_predicted = model.predict([context, question])\n","  predicted_answer = []\n","  for num, value in enumerate(answer_predicted[0]):\n","    if num < len(tokenize(original_context)):\n","      if round(value) != 0:\n","        predicted_answer.append(tokenize(original_context)[num])\n","  restored_answer = []\n","  for num, value in enumerate(answer_list[number]):\n","    if value != 0:\n","      restored_answer.append(tokenize(original_context)[num])\n","\n","  print('original_answer: ', original_answer)\n","  print('restored_answer: ', restored_answer)\n","  print('predicted_answer: ', predicted_answer)\n","  return answer_predicted\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HByyCMofMdnX","colab_type":"code","outputId":"81595e57-b745-4a19-a90f-219d0b7bc1f2","executionInfo":{"status":"ok","timestamp":1592152734164,"user_tz":-180,"elapsed":1102,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["answer_predicted = get_words(52875)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["original_context:  By 1979, with the establishment of the Foundation for Ancient Research and Mormon Studies (FARMS) as a California non-profit research institution, an effort led by Robert F. Smith began to take full account of Larson’s work and to publish a Critical Text of the Book of Mormon. Thus was born the FARMS Critical Text Project which published the first volume of the 3-volume Book of Mormon Critical Text in 1984. The third volume of that first edition was published in 1987, but was already being superseded by a second, revised edition of the entire work, greatly aided through the advice and assistance of then Yale doctoral candidate Grant Hardy, Dr. Gordon C. Thomasson, Professor John W. Welch (the head of FARMS), Professor Royal Skousen, and others too numerous to mention here. However, these were merely preliminary steps to a far more exacting and all-encompassing project.\n","restored_context:  ['By', '1979', ',', 'with', 'the', 'establishment', 'of', 'the', 'Foundation', 'for', 'Ancient', 'Research', 'and', 'Mormon', 'Studies', '(', 'FARMS', ')', 'as', 'a', 'California', 'non', '-', 'profit', 'research', 'institution', ',', 'an', 'effort', 'led', 'by', 'Robert', 'F', '.', 'Smith', 'began', 'to', 'take', 'full', 'account', 'of', 'Larson', '’', 's', 'work', 'and', 'to', 'publish', 'a', 'Critical', 'Text', 'of', 'the', 'Book', 'of', 'Mormon', '.', 'Thus', 'was', 'born', 'the', 'FARMS', 'Critical', 'Text', 'Project', 'which', 'published', 'the', 'first', 'volume', 'of', 'the', '3', '-', 'volume', 'Book', 'of', 'Mormon', 'Critical', 'Text', 'in', '1984', '.', 'The', 'third', 'volume', 'of', 'that', 'first', 'edition', 'was', 'published', 'in', '1987', ',', 'but', 'was', 'already', 'being', 'superseded', 'by', 'a', 'second', ',', 'revised', 'edition', 'of', 'the', 'entire', 'work', ',', 'greatly', 'aided', 'through', 'the', 'advice', 'and', 'assistance', 'of', 'then', 'Yale', 'doctoral', 'candidate', 'Grant', 'Hardy', ',', 'Dr', '.', 'Gordon', 'C', '.', 'Thomasson', ',', 'Professor', 'John', 'W', '.', 'Welch', '(', 'the', 'head', 'of', 'FARMS', '),', 'Professor', 'Royal', 'Skousen', ',', 'and', 'others', 'too', 'numerous', 'to', 'mention', 'here', '.', 'However', ',', 'these', 'were', 'merely', 'preliminary', 'steps', 'to', 'a', 'far', 'more', 'exacting', 'and', 'all', '-', 'encompassing', 'project', '.']\n","original_question:  Was the third volume of the first edition deemed sufficient?\n","restored_question:  ['Was', 'the', 'third', 'volume', 'of', 'the', 'first', 'edition', 'deemed', 'sufficient', '?']\n","original_answer:  The third volume of that first edition was published in 1987, but was already being superseded by a second, revised edition of the entire work\n","restored_answer:  ['The', 'third', 'volume', 'of', 'that', 'first', 'edition', 'was', 'published', 'in', '1987', ',', 'but', 'was', 'already', 'being', 'superseded', 'by', 'a', 'second', ',', 'revised', 'edition', 'of', 'the', 'entire', 'work']\n","predicted_answer:  ['volume', 'of', 'that', 'first', 'edition', 'was', 'published', 'in', '1987', ',', 'but', 'was', 'already', 'being', 'superseded', 'by', 'a', 'second', ',', 'revised', 'edition', 'of']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yt4AF5vmMx_3","colab_type":"code","outputId":"436f9624-9ebd-4baf-f912-ec3a78f61b6a","executionInfo":{"status":"ok","timestamp":1592142089664,"user_tz":-180,"elapsed":762,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(np.mean(answer_predicted[0]))\n","np.max(answer_predicted[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.022625238\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.83646905"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"cKuA2uk5Jp0g","colab_type":"code","colab":{}},"source":["json_file = '/content/drive/My Drive/final_project/model.json'\n","model_json = model.to_json()\n","\n","with open(json_file, 'w') as f:\n","  f.write(model_json)"],"execution_count":0,"outputs":[]}]}