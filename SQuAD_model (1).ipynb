{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SQuAD_model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1yRzWdXgl0LMG4wX8qJriGE1FU8mT_bKk","authorship_tag":"ABX9TyPH7Q69dRaTHF2g5iJu4INt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xvYmvWsFh_pW","colab_type":"code","outputId":"b54d3db7-9974-4a84-b21d-25528e096130","executionInfo":{"status":"ok","timestamp":1591596067243,"user_tz":-180,"elapsed":912,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pickle\n","\n","import re\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","import json\n","\n","from keras import layers\n","from keras.layers import recurrent\n","from keras.layers.embeddings import Embedding\n","from keras.models import Model\n","\n","import tensorflow as tf\n","\n","from sklearn.metrics import hamming_loss, precision_score,\\\n","                             recall_score, accuracy_score\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FnPjeq_UCfiY","colab_type":"code","colab":{}},"source":["def tokenize(sent):\n","  return [x.strip() for x in re.split(r'(\\W+)', sent) if x.strip()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"04eRKr61iNrv","colab_type":"code","outputId":"b72497f4-3a54-4c55-f431-773d1247cbf1","executionInfo":{"status":"ok","timestamp":1591521848464,"user_tz":-180,"elapsed":9267,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with open('drive/My Drive/final_project/train-v2.0.json', 'r') as f:\n","  content = json.loads(f.read())\n","type(content)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"WbaD0NJViWn4","colab_type":"code","outputId":"988fdfca-385e-434b-b730-152e876a161e","executionInfo":{"status":"ok","timestamp":1591420360511,"user_tz":-180,"elapsed":805,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(content.keys())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dict_keys(['version', 'data'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e1Q-1EUKicKS","colab_type":"code","outputId":"96f4fff1-f9c2-4396-d1c1-21ea477c0621","executionInfo":{"status":"ok","timestamp":1591521853507,"user_tz":-180,"elapsed":1083,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data = content['data']\n","data[0].keys()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['title', 'paragraphs'])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"sNt55UuKiiX9","colab_type":"code","outputId":"bf522a2d-b10c-4c95-b72c-7d061b6acee4","executionInfo":{"status":"ok","timestamp":1591420365508,"user_tz":-180,"elapsed":789,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":751}},"source":["data[320]['paragraphs'][0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'context': 'The modern English word green comes from the Middle English and Anglo-Saxon word grene, from the same Germanic root as the words \"grass\" and \"grow\". It is the color of living grass and leaves and as a result is the color most associated with springtime, growth and nature. By far the largest contributor to green in nature is chlorophyll, the chemical by which plants photosynthesize and convert sunlight into chemical energy. Many creatures have adapted to their green environments by taking on a green hue themselves as camouflage. Several minerals have a green color, including the emerald, which is colored green by its chromium content.',\n"," 'qas': [{'answers': [{'answer_start': 326, 'text': 'chlorophyll'}],\n","   'id': '5729604faf94a219006aa341',\n","   'is_impossible': False,\n","   'question': 'What, in nature, is most likely to make things green?'},\n","  {'answers': [{'answer_start': 522, 'text': 'camouflage'}],\n","   'id': '5729604faf94a219006aa342',\n","   'is_impossible': False,\n","   'question': 'For what do some animals use the color green?'},\n","  {'answers': [{'answer_start': 624, 'text': 'chromium'}],\n","   'id': '5729604faf94a219006aa343',\n","   'is_impossible': False,\n","   'question': 'What chemical causes emeralds to be green?'},\n","  {'answers': [{'answer_start': 81, 'text': 'grene'}],\n","   'id': '5729604faf94a219006aa344',\n","   'is_impossible': False,\n","   'question': 'From which Middle English and Anglo-Saxon word is green derived?'},\n","  {'answers': [],\n","   'id': '5a74990f42eae6001a389a08',\n","   'is_impossible': True,\n","   'plausible_answers': [{'answer_start': 81, 'text': 'grene'}],\n","   'question': 'What is the Germanic root word meaning grass?'},\n","  {'answers': [],\n","   'id': '5a74990f42eae6001a389a09',\n","   'is_impossible': True,\n","   'plausible_answers': [{'answer_start': 326, 'text': 'chlorophyll'}],\n","   'question': 'What chemical for converting sunlight is found in emeralds?'},\n","  {'answers': [],\n","   'id': '5a74990f42eae6001a389a0a',\n","   'is_impossible': True,\n","   'plausible_answers': [{'answer_start': 81, 'text': 'grene'}],\n","   'question': 'What word came from the English word \"green\"?'},\n","  {'answers': [],\n","   'id': '5a74990f42eae6001a389a0b',\n","   'is_impossible': True,\n","   'plausible_answers': [{'answer_start': 410, 'text': 'chemical energy'}],\n","   'question': 'What do plants convert chlorophyll into?'},\n","  {'answers': [],\n","   'id': '5a74990f42eae6001a389a0c',\n","   'is_impossible': True,\n","   'plausible_answers': [{'answer_start': 368, 'text': 'photosynthesize'}],\n","   'question': 'What process does chromium allow plants to do?'}]}"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"5qDF-uNZdjj8","colab_type":"code","colab":{}},"source":["def parse_data(data):\n","  vocab_set = set()\n","  vocab = {}\n","  triplex_list = []\n","  context_list = []\n","  question_list = []\n","  answer_list = []\n","\n","  # Context and questions extracting\n","  for topic in data:\n","    for part in topic['paragraphs']:\n","      blocks = part['qas']\n","      for block in blocks:\n","        context = part['context']\n","        vocab_set |= set(tokenize(context))\n","        context_list.append(context)\n","        vocab_set |= set(tokenize(block['question']))\n","        question_list.append(block['question'])\n","\n","  # Making dictionary with shape {'token': number}, where numbers are in range 1..\n","  i = 1\n","  for token in vocab_set:\n","    vocab[token] = i\n","    i += 1\n","  \n","  # Context vectorization and finding of context_maxlen\n","  context_vectors = []\n","  context_maxlen = 0\n","  for context in context_list:\n","    vectorized_context = []\n","    tokens = tokenize(context)\n","    for token in tokens:\n","      vectorized_context.append(vocab[token])\n","    context_vectors.append(vectorized_context)\n","    if len(tokens) > context_maxlen:\n","      context_maxlen = len(tokens)\n","  context_vectors = pad_sequences(context_vectors, maxlen=context_maxlen, padding='post')\n","\n","  # Answers extracting and vectorization\n","  for topic in data:\n","    for part in topic['paragraphs']:\n","      blocks = part['qas']\n","      for block in blocks:\n","        answer = np.zeros(context_maxlen + 1)\n","        if len(block['answers']) == 1:\n","          answer_start = block['answers'][0]['answer_start']\n","          text = block['answers'][0]['text']\n","          answer[answer_start:answer_start + len(text)] = 1\n","        answer_list.append(answer)\n","    answer_vectors = pad_sequences(answer_list, maxlen=context_maxlen, padding='post')\n","  \n","  # Question vectorization and question_maxlen finding \n","  question_vectors = []\n","  question_maxlen = 0\n","  for question in question_list:\n","    vectorized_question = []\n","    tokens = tokenize(question)\n","    for token in tokens:\n","      vectorized_question.append(vocab[token])\n","    question_vectors.append(vectorized_question)\n","    if len(tokens) > question_maxlen:\n","      question_maxlen = len(tokens)\n","  question_vectors = pad_sequences(question_vectors, maxlen=question_maxlen, padding='post')\n","\n","\n","  return context_vectors, question_vectors, answer_vectors, vocab, context_maxlen, question_maxlen"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7g1W5GRitqg","colab_type":"code","colab":{}},"source":["context_vectors, question_vectors, answer_vectors, vocab, context_maxlen, question_maxlen = parse_data(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"chDAA01GYvPm","colab_type":"code","colab":{}},"source":["with open('drive/My Drive/final_project/context_vectors.pickle', 'wb') as f:\n","  pickle.dump(context_vectors, f)\n","with open('drive/My Drive/final_project/question_vectors.pickle', 'wb') as f:\n","  pickle.dump(question_vectors, f)\n","with open('drive/My Drive/final_project/answer_vectors.pickle', 'wb') as f:\n","  pickle.dump(answer_vectors, f)\n","other = {'vocab': vocab, 'context_maxlen': context_maxlen, 'question_maxlen': question_maxlen}\n","with open('drive/My Drive/final_project/other.pickle', 'wb') as f:\n","  pickle.dump(other, f)\n","\n","#with open(PIK, \"wb\") as f:\n","#    pickle.dump(len(data), f)\n","#    for value in data:\n","#        pickle.dump(value, f)\n","#data2 = []\n","#with open(PIK, \"rb\") as f:\n","#    for _ in range(pickle.load(f)):\n","#        data2.append(pickle.load(f))\n","#print data2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqsPDuJPdT4I","colab_type":"code","colab":{}},"source":["with open('drive/My Drive/final_project/context_vectors.pickle', 'rb') as f:\n","  context_vectors = pickle.load(f)\n","with open('drive/My Drive/final_project/question_vectors.pickle', 'rb') as f:\n","  question_vectors = pickle.load(f)\n","with open('drive/My Drive/final_project/answer_vectors.pickle', 'rb') as f:\n","  answer_vectors = pickle.load(f)\n","with open('drive/My Drive/final_project/other.pickle', 'rb') as f:\n","  other = pickle.load(f)\n","vocab = other['vocab']\n","context_maxlen = other['context_maxlen']\n","question_maxlen = other['question_maxlen']\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5M5um3tYbCq-","colab_type":"code","outputId":"751a85dc-c07b-49ab-ef29-40d297195a7a","executionInfo":{"status":"ok","timestamp":1591457811753,"user_tz":-180,"elapsed":951,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["print('context ', type(context_vectors))\n","print('question ', type(question_vectors))\n","print('answer ', type(answer_vectors))\n","print('context ', context_vectors.shape)\n","print('question ', question_vectors.shape)\n","print('answer ', answer_vectors.shape)\n","print('context_maxlen= ', context_maxlen)\n","print('question_maxlen= ', question_maxlen)\n","print('Length of vocabulary= ', len(vocab))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["context  <class 'numpy.ndarray'>\n","question  <class 'numpy.ndarray'>\n","answer  <class 'numpy.ndarray'>\n","context  (130319, 844)\n","question  (130319, 60)\n","answer  (130319, 844)\n","context_maxlen=  844\n","question_maxlen=  60\n","Length of vocabulary=  99372\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dw2ht1cvrn1G","colab_type":"code","outputId":"2c01628a-b794-4206-9e3d-92a2e7dd66eb","executionInfo":{"status":"ok","timestamp":1591592403774,"user_tz":-180,"elapsed":1247,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["from sklearn.model_selection import train_test_split\n","context_train, context_test, question_train, question_test,\\\n","  answer_train, answer_test = train_test_split(context_vectors,\n","  question_vectors, answer_vectors, test_size=0.2, random_state=42)\n","\n","print('context_train', type(context_train), context_train.shape)\n","print('context_test', type(context_test), context_test.shape)\n","print('question_train', type(question_train), question_train.shape)\n","print('question_test', type(question_test), question_test.shape)\n","print('answer_train', type(answer_train), answer_train.shape)\n","print('answer_test', type(answer_test), answer_test.shape)\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["context_train <class 'numpy.ndarray'> (104255, 844)\n","context_test <class 'numpy.ndarray'> (26064, 844)\n","question_train <class 'numpy.ndarray'> (104255, 60)\n","question_test <class 'numpy.ndarray'> (26064, 60)\n","answer_train <class 'numpy.ndarray'> (104255, 844)\n","answer_test <class 'numpy.ndarray'> (26064, 844)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2TCbXj-_49vc","colab_type":"code","outputId":"dd69ba79-ab06-411b-f5a1-7fc45f555493","executionInfo":{"status":"ok","timestamp":1591592440703,"user_tz":-180,"elapsed":847,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["RNN = recurrent.LSTM\n","EMBED_HIDDEN_SIZE = 100\n","CONTEXT_HIDDEN_SIZE = 200\n","QUESTION_HIDDEN_SIZE = 200\n","BATCH_SIZE = 32\n","EPOCHS = 4\n","vocab_size = len(vocab) + 1\n","print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n","                                                           EMBED_HIDDEN_SIZE,\n","                                                           CONTEXT_HIDDEN_SIZE,\n","                                                           QUESTION_HIDDEN_SIZE))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 100, 200, 200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xZ1nz-DTQXUt","colab_type":"code","colab":{}},"source":["def label_based_metrics(y_true, y_pred):\n","  acc = []\n","  hamm = []\n","  prec = []\n","  rec = []\n","  for i in range(len(y_true)):\n","    acc_i = accuracy_score(y_true[i], y_pred[i])\n","    hamm_i = hamming_loss(y_true[i], y_pred[i])\n","    prec_i = precision_score(y_true[i], y_pred[i], average='weighted')\n","    rec_i = recall_score(y_true[i], y_pred[i], average='weighted')\n","    acc.append(acc_i)\n","    hamm.append(hamm_i)\n","    prec.append(prec_i)\n","    rec.append(rec)\n","  label_based_accuracy = np.mean(acc)\n","  label_based_hamming_loss = np.mean(hamm)\n","  label_based_precision = np.mean(prec)\n","  label_based_recall = np.mean(rec)\n","  return label_based_accuracy, label_based_hamming_loss, \\\n","          label_based_precision, label_based_recall"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJzXKnCb5eXQ","colab_type":"code","outputId":"7f9cfa0d-ba5b-472f-f181-bfe4ba6aed1b","executionInfo":{"status":"error","timestamp":1591601660063,"user_tz":-180,"elapsed":1173,"user":{"displayName":"Евгений Мотылев","photoUrl":"","userId":"00574812871737722118"}},"colab":{"base_uri":"https://localhost:8080/","height":412}},"source":["print('Build model...')\n","\n","context = layers.Input(shape=(context_maxlen,), dtype='int32')\n","encoded_context = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(context)\n","encoded_context = RNN(CONTEXT_HIDDEN_SIZE)(encoded_context)\n","\n","question = layers.Input(shape=(question_maxlen,), dtype='int32')\n","encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n","encoded_question = RNN(QUESTION_HIDDEN_SIZE)(encoded_question)\n","\n","merged = layers.concatenate([encoded_context, encoded_question])\n","preds = layers.Dense(context_maxlen, activation='sigmoid')(merged)\n","\n","model = Model([context, question], preds)\n","model.compile(optimizer='rmsprop',\n","              loss=tf.nn.sigmoid_cross_entropy_with_logits,\n","              metrics=['accuracy', label_based_metrics])\n","\n","\n","model.summary()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Build model...\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-034587020edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m model.compile(optimizer='rmsprop',\n\u001b[1;32m     16\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m               metrics=['accuracy', label_based_metrics])\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mskip_target_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             masks=masks)\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Compute total loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_metrics\u001b[0;34m(self, outputs, targets, skip_target_masks, sample_weights, masks)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                 self._handle_per_output_metrics(\n\u001b[0;32m--> 871\u001b[0;31m                     self._per_output_metrics[i], target, output, output_mask)\n\u001b[0m\u001b[1;32m    872\u001b[0m                 self._handle_per_output_metrics(\n\u001b[1;32m    873\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_per_output_weighted_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_per_output_metrics\u001b[0;34m(self, metrics_dict, y_true, y_pred, mask, weights)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 training_utils.call_metric_function(\n\u001b[0;32m--> 842\u001b[0;31m                     metric_fn, y_true, y_pred, weights=weights, mask=mask)\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     def _handle_metrics(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcall_metric_function\u001b[0;34m(metric_fn, y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For TF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mmetric_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Decorated function with `add_update()`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_or_expand_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         return super(MeanMetricWrapper, self).update_state(\n\u001b[1;32m    320\u001b[0m             matches, sample_weight=sample_weight)\n","\u001b[0;32m<ipython-input-24-909eae97746f>\u001b[0m in \u001b[0;36mlabel_based_metrics\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0macc_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mhamm_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhamming_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m     raise TypeError(\"len is not well defined for symbolic Tensors. ({}) \"\n\u001b[1;32m    753\u001b[0m                     \u001b[0;34m\"Please call `x.shape` rather than `len(x)` for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                     \"shape information.\".format(self.name))\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: len is not well defined for symbolic Tensors. (dense_11_target:0) Please call `x.shape` rather than `len(x)` for shape information."]}]},{"cell_type":"code","metadata":{"id":"cAqoCe5lVcWO","colab_type":"code","colab":{}},"source":["from keras.callbacks import ModelCheckpoint\n","\n","callback = ModelCheckpoint(filepath='/content/drive/My Drive/final_project/weights_file_1',\n","              monitor='val_loss',\n","              mode='auto',\n","              save_best_only=True)\n","\n","print('Training')\n","history = model.fit([context_train, question_train], answer_train,\n","          batch_size=BATCH_SIZE,\n","          epochs=EPOCHS,\n","          validation_split=0.05,\n","          callbacks=[callback])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6Ek-w2KUXHG","colab_type":"code","colab":{}},"source":["accuracy=history.history['accuracy']\n","val_accuracy=history.history['val_accuracy']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","print('accuracy= ', mse)\n","print('val_accuracy= ', val_mse)\n","print('loss= ', loss)\n","print('val_loss= ', val_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJdeW7ZCdePE","colab_type":"code","colab":{}},"source":["print('Evaluation')\n","loss, accuracy = model.evaluate([context_test, question_test], answer_test,\n","                           batch_size=BATCH_SIZE)\n","print('Test loss / test mse = {:.4f} / {:.4f}'.format(loss, accuracy))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bJytOWOkmll","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import model_from_json\n","\n","json_file = '/content/drive/My Drive/final_project/model.json'\n","model_json_1 = model.to_json()\n","\n","with open(json_file, 'w') as f:\n","  f.write(model_json_1)\n"],"execution_count":0,"outputs":[]}]}